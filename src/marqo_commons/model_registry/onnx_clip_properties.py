from typing import Dict, Optional, List, Tuple

from marqo_commons.model_registry.model_properties_object import ModelProperties, VectorNumericType, Modality, ModelType


class OnnxClipModelProperties(ModelProperties):
    vector_numeric_type: VectorNumericType = VectorNumericType.float32
    modality: List[Modality] = [Modality.text, Modality.image]
    type: ModelType = ModelType.clip_onnx
    repo_id: str
    visual_file: str
    textual_file: str
    resolution: int
    pretrained: str = None
    image_mean: Optional[Tuple[float, ...]] = None
    image_std: Optional[Tuple[float, ...]] = None


def _get_onnx_clip_properties() -> Dict:
    ONNX_CLIP_MODEL_PROPERTIES = {
        "onnx32/openai/ViT-L/14": vars(OnnxClipModelProperties(
            name="onnx32/openai/ViT-L/14",
            memory_size=1.5,
            dimensions=768,
            notes="the onnx float32 version of openai ViT-L/14",
            repo_id="Marqo/onnx-openai-ViT-L-14",
            visual_file="onnx32-openai-ViT-L-14-visual.onnx",
            textual_file="onnx32-openai-ViT-L-14-textual.onnx",
            resolution=224,
        )),
        "onnx16/openai/ViT-L/14": vars(OnnxClipModelProperties(
            name="onnx16/openai/ViT-L/14",
            memory_size=1.5,
            dimensions=768,
            notes="the onnx float16 version of openai ViT-L/14",
            repo_id="Marqo/onnx-openai-ViT-L-14",
            visual_file="onnx16-openai-ViT-L-14-visual.onnx",
            textual_file="onnx16-openai-ViT-L-14-textual.onnx",
            resolution=224,
        )),
        "onnx32/open_clip/ViT-L-14/openai": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/ViT-L-14/openai",
            memory_size=1.5,
            dimensions=768,
            notes="the onnx float32 version of open_clip ViT-L-14/openai",
            repo_id="Marqo/onnx-open_clip-ViT-L-14",
            visual_file="onnx32-open_clip-ViT-L-14-openai-visual.onnx",
            textual_file="onnx32-open_clip-ViT-L-14-openai-textual.onnx",
            resolution=224,
            pretrained="openai",
        )),
        "onnx16/open_clip/ViT-L-14/openai": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/ViT-L-14/openai",
            memory_size=1.5,
            dimensions=768,
            notes="the onnx float16 version of open_clip ViT-L-14/openai",
            repo_id="Marqo/onnx-open_clip-ViT-L-14",
            visual_file="onnx16-open_clip-ViT-L-14-openai-visual.onnx",
            textual_file="onnx16-open_clip-ViT-L-14-openai-textual.onnx",
            resolution=224,
            pretrained="openai",
        )),
        "onnx32/open_clip/ViT-L-14/laion400m_e32": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/ViT-L-14/laion400m_e32",
            memory_size=1.5,
            dimensions=768,
            notes="the onnx float32 version of open_clip ViT-L-14/lainon400m_e32",
            repo_id="Marqo/onnx-open_clip-ViT-L-14",
            visual_file="onnx32-open_clip-ViT-L-14-laion400m_e32-visual.onnx",
            textual_file="onnx32-open_clip-ViT-L-14-laion400m_e32-textual.onnx",
            resolution=224,
            pretrained="laion400m_e32",
        )),
        "onnx16/open_clip/ViT-L-14/laion400m_e32": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/ViT-L-14/laion400m_e32",
            memory_size=1.5,
            dimensions=768,
            notes="the onnx float16 version of open_clip ViT-L-14/lainon400m_e32",
            repo_id="Marqo/onnx-open_clip-ViT-L-14",
            visual_file="onnx16-open_clip-ViT-L-14-laion400m_e32-visual.onnx",
            textual_file="onnx16-open_clip-ViT-L-14-laion400m_e32-textual.onnx",
            resolution=224,
            pretrained="laion400m_e32",
        )),
        "onnx32/open_clip/ViT-L-14/laion2b_s32b_b82k": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/ViT-L-14/laion2b_s32b_b82k",
            memory_size=1.5,
            dimensions=768,
            notes="the onnx float32 version of open_clip ViT-L-14/laion2b_s32b_b82k",
            repo_id="Marqo/onnx-open_clip-ViT-L-14",
            visual_file="onnx32-open_clip-ViT-L-14-laion2b_s32b_b82k-visual.onnx",
            textual_file="onnx32-open_clip-ViT-L-14-laion2b_s32b_b82k-textual.onnx",
            resolution=224,
            pretrained="laionb_s32b_b82k",
            image_mean=(0.5, 0.5, 0.5),
            image_std=(0.5, 0.5, 0.5),
        )),
        "onnx16/open_clip/ViT-L-14/laion2b_s32b_b82k": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/ViT-L-14/laion2b_s32b_b82k",
            memory_size=1.5,
            dimensions=768,
            notes="the onnx float16 version of open_clip ViT-L-14/laion2b_s32b_b82k",
            repo_id="Marqo/onnx-open_clip-ViT-L-14",
            visual_file="onnx16-open_clip-ViT-L-14-laion2b_s32b_b82k-visual.onnx",
            textual_file="onnx16-open_clip-ViT-L-14-laion2b_s32b_b82k-textual.onnx",
            resolution=224,
            pretrained="laionb_s32b_b82k",
            image_mean=(0.5, 0.5, 0.5),
            image_std=(0.5, 0.5, 0.5),
        )),
        "onnx32/open_clip/ViT-L-14-336/openai": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/ViT-L-14-336/openai",
            memory_size=1.5,
            dimensions=768,
            notes="the onnx float32 version of open_clip ViT-L-14-336/openai",
            repo_id="Marqo/onnx-open_clip-ViT-L-14-336",
            visual_file="onnx32-open_clip-ViT-L-14-336-openai-visual.onnx",
            textual_file="onnx32-open_clip-ViT-L-14-336-openai-textual.onnx",
            resolution=336,
            pretrained="openai",
        )),
        "onnx16/open_clip/ViT-L-14-336/openai": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/ViT-L-14-336/openai",
            memory_size=1.5,
            dimensions=768,
            notes="the onnx float16 version of open_clip ViT-L-14-336/openai",
            repo_id="Marqo/onnx-open_clip-ViT-L-14-336",
            visual_file="onnx16-open_clip-ViT-L-14-336-openai-visual.onnx",
            textual_file="onnx16-open_clip-ViT-L-14-336-openai-textual.onnx",
            resolution=336,
            pretrained="openai",
        )),
        "onnx32/open_clip/ViT-B-32/openai": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/ViT-B-32/openai",
            memory_size=1,
            dimensions=512,
            notes="the onnx float32 version of open_clip ViT-B-32/openai",
            repo_id="Marqo/onnx-open_clip-ViT-B-32",
            visual_file="onnx32-open_clip-ViT-B-32-openai-visual.onnx",
            textual_file="onnx32-open_clip-ViT-B-32-openai-textual.onnx",
            resolution=224,
            pretrained="openai",
        )),
        "onnx16/open_clip/ViT-B-32/openai": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/ViT-B-32/openai",
            memory_size=1,
            dimensions=512,
            notes="the onnx float16 version of open_clip ViT-B-32/openai",
            repo_id="Marqo/onnx-open_clip-ViT-B-32",
            visual_file="onnx16-open_clip-ViT-B-32-openai-visual.onnx",
            textual_file="onnx16-open_clip-ViT-B-32-openai-textual.onnx",
            resolution=224,
            pretrained="openai",
        )),
        "onnx32/open_clip/ViT-B-32/laion400m_e31": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/ViT-B-32/laion400m_e31",
            memory_size=1,
            dimensions=512,
            notes="the onnx float32 version of open_clip ViT-B-32/laion400m_e31",
            repo_id="Marqo/onnx-open_clip-ViT-B-32",
            visual_file="onnx32-open_clip-ViT-B-32-laion400m_e31-visual.onnx",
            textual_file="onnx32-open_clip-ViT-B-32-laion400m_e31-textual.onnx",
            resolution=224,
            pretrained="laion400m_e31",
        )),
        "onnx16/open_clip/ViT-B-32/laion400m_e31": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/ViT-B-32/laion400m_e31",
            memory_size=1,
            dimensions=512,
            notes="the onnx float16 version of open_clip ViT-B-32/laion400m_e31",
            repo_id="Marqo/onnx-open_clip-ViT-B-32",
            visual_file="onnx16-open_clip-ViT-B-32-laion400m_e31-visual.onnx",
            textual_file="onnx16-open_clip-ViT-B-32-laion400m_e31-textual.onnx",
            resolution=224,
            pretrained="laion400m_e31",
        )),
        "onnx32/open_clip/ViT-B-32/laion400m_e32": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/ViT-B-32/laion400m_e32",
            memory_size=1,
            dimensions=512,
            notes="the onnx float32 version of open_clip ViT-B-32/laion400m_e32",
            repo_id="Marqo/onnx-open_clip-ViT-B-32",
            visual_file="onnx32-open_clip-ViT-B-32-laion400m_e32-visual.onnx",
            textual_file="onnx32-open_clip-ViT-B-32-laion400m_e32-textual.onnx",
            resolution=224,
            pretrained="laion400m_e32",
        )),
        "onnx16/open_clip/ViT-B-32/laion400m_e32": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/ViT-B-32/laion400m_e32",
            memory_size=1,
            dimensions=512,
            notes="the onnx float16 version of open_clip ViT-B-32/laion400m_e32",
            repo_id="Marqo/onnx-open_clip-ViT-B-32",
            visual_file="onnx16-open_clip-ViT-B-32-laion400m_e32-visual.onnx",
            textual_file="onnx16-open_clip-ViT-B-32-laion400m_e32-textual.onnx",
            resolution=224,
            pretrained="laion400m_e32",
        )),
        "onnx32/open_clip/ViT-B-32/laion2b_e16": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/ViT-B-32/laion2b_e16",
            memory_size=1,
            dimensions=512,
            notes="the onnx float32 version of open_clip ViT-B-32/laion2b_e16",
            repo_id="Marqo/onnx-open_clip-ViT-B-32",
            visual_file="onnx32-open_clip-ViT-B-32-laion2b_e16-visual.onnx",
            textual_file="onnx32-open_clip-ViT-B-32-laion2b_e16-textual.onnx",
            resolution=224,
            pretrained="laion2b_e16",
        )),
        "onnx16/open_clip/ViT-B-32/laion2b_e16": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/ViT-B-32/laion2b_e16",
            memory_size=1,
            dimensions=512,
            notes="the onnx float16 version of open_clip ViT-B-32/laion2b_e16",
            repo_id="Marqo/onnx-open_clip-ViT-B-32",
            visual_file="onnx16-open_clip-ViT-B-32-laion2b_e16-visual.onnx",
            textual_file="onnx16-open_clip-ViT-B-32-laion2b_e16-textual.onnx",
            resolution=224,
            pretrained="laion2b_e16",
        )),
        "onnx32/open_clip/ViT-B-32-quickgelu/openai": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/ViT-B-32-quickgelu/openai",
            memory_size=1,
            dimensions=512,
            notes="the onnx float32 version of open_clip ViT-B-32-quickgelu/openai",
            repo_id="Marqo/onnx-open_clip-ViT-B-32-quickgelu",
            visual_file="onnx32-open_clip-ViT-B-32-quickgelu-openai-visual.onnx",
            textual_file="onnx32-open_clip-ViT-B-32-quickgelu-openai-textual.onnx",
            resolution=224,
            pretrained="openai",
        )),
        "onnx16/open_clip/ViT-B-32-quickgelu/openai": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/ViT-B-32-quickgelu/openai",
            memory_size=1,
            dimensions=512,
            notes="the onnx float16 version of open_clip ViT-B-32-quickgelu/openai",
            repo_id="Marqo/onnx-open_clip-ViT-B-32-quickgelu",
            visual_file="onnx16-open_clip-ViT-B-32-quickgelu-openai-visual.onnx",
            textual_file="onnx16-open_clip-ViT-B-32-quickgelu-openai-textual.onnx",
            resolution=224,
            pretrained="openai",
        )),
        "onnx32/open_clip/ViT-B-32-quickgelu/laion400m_e31": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/ViT-B-32-quickgelu/laion400m_e31",
            memory_size=1,
            dimensions=512,
            notes="the onnx float32 version of open_clip ViT-B-32-quickgelu/laion400m_e31",
            repo_id="Marqo/onnx-open_clip-ViT-B-32-quickgelu",
            visual_file="onnx32-open_clip-ViT-B-32-quickgelu-laion400m_e31-visual.onnx",
            textual_file="onnx32-open_clip-ViT-B-32-quickgelu-laion400m_e31-textual.onnx",
            resolution=224,
            pretrained="laion400m_e31",
        )),
        "onnx16/open_clip/ViT-B-32-quickgelu/laion400m_e31": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/ViT-B-32-quickgelu/laion400m_e31",
            memory_size=1,
            dimensions=512,
            notes="the onnx float16 version of open_clip ViT-B-32-quickgelu/laion400m_e31",
            repo_id="Marqo/onnx-open_clip-ViT-B-32-quickgelu",
            visual_file="onnx16-open_clip-ViT-B-32-quickgelu-laion400m_e31-visual.onnx",
            textual_file="onnx16-open_clip-ViT-B-32-quickgelu-laion400m_e31-textual.onnx",
            resolution=224,
            pretrained="laion400m_e31",
                )),
        "onnx16/open_clip/ViT-B-32-quickgelu/laion400m_e32": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/ViT-B-32-quickgelu/laion400m_e32",
            memory_size=1,
            dimensions=512,
            notes="the onnx float16 version of open_clip ViT-B-32-quickgelu/laion400m_e32",
            repo_id="Marqo/onnx-open_clip-ViT-B-32-quickgelu",
            visual_file="onnx16-open_clip-ViT-B-32-quickgelu-laion400m_e32-visual.onnx",
            textual_file="onnx16-open_clip-ViT-B-32-quickgelu-laion400m_e32-textual.onnx",
            resolution=224,
            pretrained="laion400m_e32",
        )),
        "onnx32/open_clip/ViT-B-32-quickgelu/laion400m_e32": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/ViT-B-32-quickgelu/laion400m_e32",
            memory_size=1,
            dimensions=512,
            notes="the onnx float32 version of open_clip ViT-B-32-quickgelu/laion400m_e32",
            repo_id="Marqo/onnx-open_clip-ViT-B-32-quickgelu",
            visual_file="onnx32-open_clip-ViT-B-32-quickgelu-laion400m_e32-visual.onnx",
            textual_file="onnx32-open_clip-ViT-B-32-quickgelu-laion400m_e32-textual.onnx",
            resolution=224,
            pretrained="laion400m_e32",
        )),
        "onnx16/open_clip/ViT-B-16/openai": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/ViT-B-16/openai",
            memory_size=1,
            dimensions=512,
            notes="the onnx float16 version of open_clip ViT-B-16/openai",
            repo_id="Marqo/onnx-open_clip-ViT-B-16",
            visual_file="onnx16-open_clip-ViT-B-16-openai-visual.onnx",
            textual_file="onnx16-open_clip-ViT-B-16-openai-textual.onnx",
            resolution=224,
            pretrained="openai",
        )),
        "onnx32/open_clip/ViT-B-16/openai": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/ViT-B-16/openai",
            memory_size=1,
            dimensions=512,
            notes="the onnx float32 version of open_clip ViT-B-16/openai",
            repo_id="Marqo/onnx-open_clip-ViT-B-16",
            visual_file="onnx32-open_clip-ViT-B-16-openai-visual.onnx",
            textual_file="onnx32-open_clip-ViT-B-16-openai-textual.onnx",
            resolution=224,
            pretrained="openai",
        )),
        "onnx16/open_clip/ViT-B-16/laion400m_e31": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/ViT-B-16/laion400m_e31",
            memory_size=1,
            dimensions=512,
            notes="the onnx float16 version of open_clip ViT-B-16/laion400m_e31",
            repo_id="Marqo/onnx-open_clip-ViT-B-16",
            visual_file="onnx16-open_clip-ViT-B-16-laion400m_e31-visual.onnx",
            textual_file="onnx16-open_clip-ViT-B-16-laion400m_e31-textual.onnx",
            resolution=224,
            pretrained="laion400m_e31",
        )),
        "onnx32/open_clip/ViT-B-16/laion400m_e31": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/ViT-B-16/laion400m_e31",
            memory_size=1,
            dimensions=512,
            notes="the onnx float32 version of open_clip ViT-B-16/laion400m_e31",
            repo_id="Marqo/onnx-open_clip-ViT-B-16",
            visual_file="onnx32-open_clip-ViT-B-16-laion400m_e31-visual.onnx",
            textual_file="onnx32-open_clip-ViT-B-16-laion400m_e31-textual.onnx",
            resolution=224,
            pretrained="laion400m_e31",
        )),
        "onnx16/open_clip/ViT-B-16/laion400m_e32": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/ViT-B-16/laion400m_e32",
            memory_size=1,
            dimensions=512,
            notes="the onnx float16 version of open_clip ViT-B-16/laion400m_e32",
            repo_id="Marqo/onnx-open_clip-ViT-B-16",
            visual_file="onnx16-open_clip-ViT-B-16-laion400m_e32-visual.onnx",
            textual_file="onnx16-open_clip-ViT-B-16-laion400m_e32-textual.onnx",
            resolution=224,
            pretrained="laion400m_e32",
        )),
        "onnx32/open_clip/ViT-B-16/laion400m_e32": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/ViT-B-16/laion400m_e32",
            memory_size=1,
            dimensions=512,
            notes="the onnx float32 version of open_clip ViT-B-16/laion400m_e32",
            repo_id="Marqo/onnx-open_clip-ViT-B-16",
            visual_file="onnx32-open_clip-ViT-B-16-laion400m_e32-visual.onnx",
            textual_file="onnx32-open_clip-ViT-B-16-laion400m_e32-textual.onnx",
            resolution=224,
            pretrained="laion400m_e32",
        )),
        "onnx16/open_clip/ViT-B-16-plus-240/laion400m_e31": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/ViT-B-16-plus-240/laion400m_e31",
            memory_size=1,
            dimensions=640,
            notes="the onnx float16 version of open_clip ViT-B-16-plus-240/laion400m_e31",
            repo_id="Marqo/onnx-open_clip-ViT-B-16-plus-240",
            visual_file="onnx16-open_clip-ViT-B-16-plus-240-laion400m_e31-visual.onnx",
            textual_file="onnx16-open_clip-ViT-B-16-plus-240-laion400m_e31-textual.onnx",
            resolution=240,
            pretrained="laion400m_e31",
        )),
        "onnx32/open_clip/ViT-B-16-plus-240/laion400m_e31": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/ViT-B-16-plus-240/laion400m_e31",
            memory_size=1,
            dimensions=640,
            notes="the onnx float32 version of open_clip ViT-B-16-plus-240/laion400m_e31",
            repo_id="Marqo/onnx-open_clip-ViT-B-16-plus-240",
            visual_file="onnx32-open_clip-ViT-B-16-plus-240-laion400m_e31-visual.onnx",
            textual_file="onnx32-open_clip-ViT-B-16-plus-240-laion400m_e31-textual.onnx",
            resolution=240,
            pretrained="laion400m_e31",
        )),
        "onnx16/open_clip/ViT-B-16-plus-240/laion400m_e32": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/ViT-B-16-plus-240/laion400m_e32",
            memory_size=1,
            dimensions=640,
            notes="the onnx float16 version of open_clip ViT-B-16-plus-240/laion400m_e32",
            repo_id="Marqo/onnx-open_clip-ViT-B-16-plus-240",
            visual_file="onnx16-open_clip-ViT-B-16-plus-240-laion400m_e32-visual.onnx",
            textual_file="onnx16-open_clip-ViT-B-16-plus-240-laion400m_e32-textual.onnx",
            resolution=240,
            pretrained="laion400m_e32",
        )),
        "onnx32/open_clip/ViT-B-16-plus-240/laion400m_e32": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/ViT-B-16-plus-240/laion400m_e32",
            memory_size=1,
            dimensions=640,
            notes="the onnx float32 version of open_clip ViT-B-16-plus-240/laion400m_e32",
            repo_id="Marqo/onnx-open_clip-ViT-B-16-plus-240",
            visual_file="onnx32-open_clip-ViT-B-16-plus-240-laion400m_e32-visual.onnx",
            textual_file="onnx32-open_clip-ViT-B-16-plus-240-laion400m_e32-textual.onnx",
            resolution=240,
            pretrained="laion400m_e32",
        )),
        "onnx16/open_clip/ViT-H-14/laion2b_s32b_b79k": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/ViT-H-14/laion2b_s32b_b79k",
            memory_size=5,
            dimensions=1024,
            notes="the onnx float16 version of open_clip ViT-H-14/laion2b_s32b_b79k",
            repo_id="Marqo/onnx-open_clip-ViT-H-14",
            visual_file="onnx16-open_clip-ViT-H-14-laion2b_s32b_b79k-visual.onnx",
            textual_file="onnx16-open_clip-ViT-H-14-laion2b_s32b_b79k-textual.onnx",
            resolution=224,
            pretrained="laion2b_s32b_b79k",
        )),
        "onnx32/open_clip/ViT-H-14/laion2b_s32b_b79k": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/ViT-H-14/laion2b_s32b_b79k",
            memory_size=5,
            dimensions=1024,
            notes="the onnx float32 version of open_clip ViT-H-14/laion2b_s32b_b79k",
            repo_id="Marqo/onnx-open_clip-ViT-H-14",
            visual_file="onnx32-open_clip-ViT-H-14-laion2b_s32b_b79k-visual.zip",
            textual_file="onnx32-open_clip-ViT-H-14-laion2b_s32b_b79k-textual.onnx",
            resolution=224,
            pretrained="laion2b_s32b_b79k",
        )),
        "onnx16/open_clip/ViT-g-14/laion2b_s12b_b42k": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/ViT-g-14/laion2b_s12b_b42k",
            memory_size=5,
            dimensions=1024,
            notes="the onnx float16 version of open_clip ViT-g-14/laion2b_s12b_b42k",
            repo_id="Marqo/onnx-open_clip-ViT-g-14",
            visual_file="onnx16-open_clip-ViT-g-14-laion2b_s12b_b42k-visual.onnx",
            textual_file="onnx16-open_clip-ViT-g-14-laion2b_s12b_b42k-textual.onnx",
            resolution=224,
            pretrained="laion2b_s12b_b42k",
        )),
        "onnx32/open_clip/ViT-g-14/laion2b_s12b_b42k": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/ViT-g-14/laion2b_s12b_b42k",
            memory_size=5,
            dimensions=1024,
            notes="the onnx float32 version of open_clip ViT-g-14/laion2b_s12b_b42k",
            repo_id="Marqo/onnx-open_clip-ViT-g-14",
            visual_file="onnx32-open_clip-ViT-g-14-laion2b_s12b_b42k-visual.zip",
            textual_file="onnx32-open_clip-ViT-g-14-laion2b_s12b_b42k-textual.onnx",
            resolution=224,
            pretrained="laion2b_s12b_b42k",
        )),
        "onnx16/open_clip/RN50/openai": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/RN50/openai",
            memory_size=1,
            dimensions=1024,
            notes="the onnx float16 version of open_clip RN50/openai",
            repo_id="Marqo/onnx-open_clip-RN50",
            visual_file="onnx16-open_clip-RN50-openai-visual.onnx",
            textual_file="onnx16-open_clip-RN50-openai-textual.onnx",
            resolution=224,
            pretrained="openai",
        )),
        "onnx32/open_clip/RN50/openai": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/RN50/openai",
            memory_size=1,
            dimensions=1024,
            notes="the onnx float32 version of open_clip RN50/openai",
            repo_id="Marqo/onnx-open_clip-RN50",
            visual_file="onnx32-open_clip-RN50-openai-visual.onnx",
            textual_file="onnx32-open_clip-RN50-openai-textual.onnx",
            resolution=224,
            pretrained="openai",
        )),
        "onnx16/open_clip/RN50/yfcc15m": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/RN50/yfcc15m",
            memory_size=1,
            dimensions=1024,
            notes="the onnx float16 version of open_clip RN50/yfcc15m",
            repo_id="Marqo/onnx-open_clip-RN50",
            visual_file="onnx16-open_clip-RN50-yfcc15m-visual.onnx",
            textual_file="onnx16-open_clip-RN50-yfcc15m-textual.onnx",
            resolution=224,
            pretrained="yfcc15m",
        )),
        "onnx32/open_clip/RN50/yfcc15m": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/RN50/yfcc15m",
            memory_size=1,
            dimensions=1024,
            notes="the onnx float32 version of open_clip RN50/yfcc15m",
            repo_id="Marqo/onnx-open_clip-RN50",
            visual_file="onnx32-open_clip-RN50-yfcc15m-visual.onnx",
            textual_file="onnx32-open_clip-RN50-yfcc15m-textual.onnx",
            resolution=224,
            pretrained="yfcc15m",
        )),
        "onnx16/open_clip/RN50/cc12m": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/RN50/cc12m",
            memory_size=1,
            dimensions=1024,
            notes="the onnx float16 version of open_clip RN50/cc12m",
            repo_id="Marqo/onnx-open_clip-RN50",
            visual_file="onnx16-open_clip-RN50-cc12m-visual.onnx",
            textual_file="onnx16-open_clip-RN50-cc12m-textual.onnx",
            resolution=224,
            pretrained="cc12m",
        )),
        "onnx32/open_clip/RN50/cc12m": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/RN50/cc12m",
            memory_size=1,
            dimensions=1024,
            notes="the onnx float32 version of open_clip RN50/cc12m",
            repo_id="Marqo/onnx-open_clip-RN50",
            visual_file="onnx32-open_clip-RN50-cc12m-visual.onnx",
            textual_file="onnx32-open_clip-RN50-cc12m-textual.onnx",
            resolution=224,
            pretrained="cc12m",
        )),
        "onnx16/open_clip/RN50-quickgelu/openai": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/RN50-quickgelu/openai",
            memory_size=1,
            dimensions=1024,
            notes="the onnx float16 version of open_clip RN50-quickgelu/openai",
            repo_id="Marqo/onnx-open_clip-RN50-quickgelu",
            visual_file="onnx16-open_clip-RN50-quickgelu-openai-visual.onnx",
            textual_file="onnx16-open_clip-RN50-quickgelu-openai-textual.onnx",
            resolution=224,
            pretrained="openai",
        )),
        "onnx32/open_clip/RN50-quickgelu/openai": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/RN50-quickgelu/openai",
            memory_size=1,
            dimensions=1024,
            notes="the onnx float32 version of open_clip RN50-quickgelu/openai",
            repo_id="Marqo/onnx-open_clip-RN50-quickgelu",
            visual_file="onnx32-open_clip-RN50-quickgelu-openai-visual.onnx",
            textual_file="onnx32-open_clip-RN50-quickgelu-openai-textual.onnx",
            resolution=224,
            pretrained="openai",
        )),
        "onnx16/open_clip/RN50-quickgelu/yfcc15m": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/RN50-quickgelu/yfcc15m",
            memory_size=1,
            dimensions=1024,
            notes="the onnx float16 version of open_clip RN50-quickgelu/yfcc15m",
            repo_id="Marqo/onnx-open_clip-RN50-quickgelu",
            visual_file="onnx16-open_clip-RN50-quickgelu-yfcc15m-visual.onnx",
            textual_file="onnx16-open_clip-RN50-quickgelu-yfcc15m-textual.onnx",
            resolution=224,
            pretrained="yfcc15m",
        )),
        "onnx32/open_clip/RN50-quickgelu/yfcc15m": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/RN50-quickgelu/yfcc15m",
            memory_size=1,
            dimensions=1024,
            notes="the onnx float32 version of open_clip RN50-quickgelu/yfcc15m",
            repo_id="Marqo/onnx-open_clip-RN50-quickgelu",
            visual_file="onnx32-open_clip-RN50-quickgelu-yfcc15m-visual.onnx",
            textual_file="onnx32-open_clip-RN50-quickgelu-yfcc15m-textual.onnx",
            resolution=224,
            pretrained="yfcc15m",
        )),
        "onnx16/open_clip/RN50-quickgelu/cc12m": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/RN50-quickgelu/cc12m",
            memory_size=1,
            dimensions=1024,
            notes="the onnx float16 version of open_clip RN50-quickgelu/cc12m",
            repo_id="Marqo/onnx-open_clip-RN50-quickgelu",
            visual_file="onnx16-open_clip-RN50-quickgelu-cc12m-visual.onnx",
            textual_file="onnx16-open_clip-RN50-quickgelu-cc12m-textual.onnx",
            resolution=224,
            pretrained="cc12m",
        )),
        "onnx32/open_clip/RN50-quickgelu/cc12m": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/RN50-quickgelu/cc12m",
            memory_size=1,
            dimensions=1024,
            notes="the onnx float32 version of open_clip RN50-quickgelu/cc12m",
            repo_id="Marqo/onnx-open_clip-RN50-quickgelu",
            visual_file="onnx32-open_clip-RN50-quickgelu-cc12m-visual.onnx",
            textual_file="onnx32-open_clip-RN50-quickgelu-cc12m-textual.onnx",
            resolution=224,
            pretrained="cc12m",
        )),
        "onnx16/open_clip/RN101/openai": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/RN101/openai",
            memory_size=1,
            dimensions=512,
            notes="the onnx float16 version of open_clip RN101/openai",
            repo_id="Marqo/onnx-open_clip-RN101",
            visual_file="onnx16-open_clip-RN101-openai-visual.onnx",
            textual_file="onnx16-open_clip-RN101-openai-textual.onnx",
            resolution=224,
            pretrained="openai",
        )),
        "onnx32/open_clip/RN101/openai": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/RN101/openai",
            memory_size=1,
            dimensions=512,
            notes="the onnx float32 version of open_clip RN101/openai",
            repo_id="Marqo/onnx-open_clip-RN101",
            visual_file="onnx32-open_clip-RN101-openai-visual.onnx",
            textual_file="onnx32-open_clip-RN101-openai-textual.onnx",
            resolution=224,
            pretrained="openai",
        )),
        "onnx16/open_clip/RN101/yfcc15m": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/RN101/yfcc15m",
            memory_size=1,
            dimensions=512,
            notes="the onnx float16 version of open_clip RN101/yfcc15m",
            repo_id="Marqo/onnx-open_clip-RN101",
            visual_file="onnx16-open_clip-RN101-yfcc15m-visual.onnx",
            textual_file="onnx16-open_clip-RN101-yfcc15m-textual.onnx",
            resolution=224,
            pretrained="yfcc15m",
        )),
        "onnx32/open_clip/RN101/yfcc15m": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/RN101/yfcc15m",
            memory_size=1,
            dimensions=512,
            notes="the onnx float32 version of open_clip RN101/yfcc15m",
            repo_id="Marqo/onnx-open_clip-RN101",
            visual_file="onnx32-open_clip-RN101-yfcc15m-visual.onnx",
            textual_file="onnx32-open_clip-RN101-yfcc15m-textual.onnx",
            resolution=224,
            pretrained="yfcc15m",
        )),
        "onnx16/open_clip/RN101-quickgelu/openai": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/RN101-quickgelu/openai",
            memory_size=1,
            dimensions=512,
            notes="the onnx float16 version of open_clip RN101-quickgelu/openai",
            repo_id="Marqo/onnx-open_clip-RN101-quickgelu",
            visual_file="onnx16-open_clip-RN101-quickgelu-openai-visual.onnx",
            textual_file="onnx16-open_clip-RN101-quickgelu-openai-textual.onnx",
            resolution=224,
            pretrained="openai",
        )),
        "onnx32/open_clip/RN101-quickgelu/openai": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/RN101-quickgelu/openai",
            memory_size=1,
            dimensions=512,
            notes="the onnx float32 version of open_clip RN101-quickgelu/openai",
            repo_id="Marqo/onnx-open_clip-RN101-quickgelu",
            visual_file="onnx32-open_clip-RN101-quickgelu-openai-visual.onnx",
            textual_file="onnx32-open_clip-RN101-quickgelu-openai-textual.onnx",
            resolution=224,
            pretrained="openai",
        )),
        "onnx16/open_clip/RN101-quickgelu/yfcc15m": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/RN101-quickgelu/yfcc15m",
            memory_size=1,
            dimensions=512,
            notes="the onnx float16 version of open_clip RN101-quickgelu/yfcc15m",
            repo_id="Marqo/onnx-open_clip-RN101-quickgelu",
            visual_file="onnx16-open_clip-RN101-quickgelu-yfcc15m-visual.onnx",
            textual_file="onnx16-open_clip-RN101-quickgelu-yfcc15m-textual.onnx",
            resolution=224,
            pretrained="yfcc15m",
        )),
        "onnx32/open_clip/RN101-quickgelu/yfcc15m": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/RN101-quickgelu/yfcc15m",
            memory_size=1,
            dimensions=512,
            notes="the onnx float32 version of open_clip RN101-quickgelu/yfcc15m",
            repo_id="Marqo/onnx-open_clip-RN101-quickgelu",
            visual_file="onnx32-open_clip-RN101-quickgelu-yfcc15m-visual.onnx",
            textual_file="onnx32-open_clip-RN101-quickgelu-yfcc15m-textual.onnx",
            resolution=224,
            pretrained="yfcc15m",
        )),
        "onnx16/open_clip/RN50x4/openai": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/RN50x4/openai",
            memory_size=1,
            dimensions=640,
            notes="the onnx float16 version of open_clip RN50x4/openai",
            repo_id="Marqo/onnx-open_clip-RN50x4",
            visual_file="onnx16-open_clip-RN50x4-openai-visual.onnx",
            textual_file="onnx16-open_clip-RN50x4-openai-textual.onnx",
            resolution=288,
            pretrained="openai",
        )),
        "onnx32/open_clip/RN50x4/openai": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/RN50x4/openai",
            memory_size=1,
            dimensions=640,
            notes="the onnx float32 version of open_clip RN50x4/openai",
            repo_id="Marqo/onnx-open_clip-RN50x4",
            visual_file="onnx32-open_clip-RN50x4-openai-visual.onnx",
            textual_file="onnx32-open_clip-RN50x4-openai-textual.onnx",
            resolution=288,
            pretrained="openai",
        )),
        "onnx16/open_clip/RN50x16/openai": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/RN50x16/openai",
            memory_size=1,
            dimensions=768,
            notes="the onnx float16 version of open_clip RN50x16/openai",
            repo_id="Marqo/onnx-open_clip-RN50x16",
            visual_file="onnx16-open_clip-RN50x16-openai-visual.onnx",
            textual_file="onnx16-open_clip-RN50x16-openai-textual.onnx",
            resolution=384,
            pretrained="openai",
        )),
        "onnx32/open_clip/RN50x16/openai": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/RN50x16/openai",
            memory_size=1,
            dimensions=768,
            notes="the onnx float32 version of open_clip RN50x16/openai",
            repo_id="Marqo/onnx-open_clip-RN50x16",
            visual_file="onnx32-open_clip-RN50x16-openai-visual.onnx",
            textual_file="onnx32-open_clip-RN50x16-openai-textual.onnx",
            resolution=384,
            pretrained="openai",
        )),
        "onnx16/open_clip/RN50x64/openai": vars(OnnxClipModelProperties(
            name="onnx16/open_clip/RN50x64/openai",
            memory_size=1,
            dimensions=1024,
            notes="the onnx float16 version of open_clip RN50x64/openai",
            repo_id="Marqo/onnx-open_clip-RN50x64",
            visual_file="onnx16-open_clip-RN50x64-openai-visual.onnx",
            textual_file="onnx16-open_clip-RN50x64-openai-textual.onnx",
            resolution=448,
            pretrained="openai",
        )),
        "onnx32/open_clip/RN50x64/openai": vars(OnnxClipModelProperties(
            name="onnx32/open_clip/RN50x64/openai",
            memory_size=1,
            dimensions=1024,
            notes="the onnx float32 version of open_clip RN50x64/openai",
            repo_id="Marqo/onnx-open_clip-RN50x64",
            visual_file="onnx32-open_clip-RN50x64-openai-visual.onnx",
            textual_file="onnx32-open_clip-RN50x64-openai-textual.onnx",
            resolution=448,
            pretrained="openai",
        )),
    }

    return ONNX_CLIP_MODEL_PROPERTIES
