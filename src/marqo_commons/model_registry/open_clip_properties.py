from typing import Dict

from marqo_commons.model_registry.model_properties_object import ModelProperties, VectorNumericType, Modality, ModelType


class OpenClipModelProperties(ModelProperties):
    vector_numeric_type: VectorNumericType = VectorNumericType.float32
    modality: list[Modality] = [Modality.text, Modality.image]
    type: ModelType = ModelType.open_clip
    pretrained: str
    notes: str = ""


def _get_open_clip_properties() -> Dict:
    # use this link to find all the model_configs
    # https://github.com/mlfoundations/open_clip/tree/main/src/open_clip/model_configs
    OPEN_CLIP_MODEL_PROPERTIES = {
        "open_clip/RN50/openai": vars(OpenClipModelProperties(
            name="open_clip/RN50/openai",
            memory_size=1,
            dimensions=1024,
            notes="open_clip models",
            pretrained="openai",
        )),
        "open_clip/RN50/yfcc15m": vars(OpenClipModelProperties(
            name="open_clip/RN50/yfcc15m",
            memory_size=1,
            dimensions=1024,
            notes="open_clip models",
            pretrained="yfcc15m",
        )),
        "open_clip/RN50/cc12m": vars(OpenClipModelProperties(
            name="open_clip/RN50/cc12m",
            memory_size=1,
            dimensions=1024,
            notes="open_clip models",
            pretrained="cc12m",
        )),
        "open_clip/RN50-quickgelu/openai": vars(OpenClipModelProperties(
            name="open_clip/RN50-quickgelu/openai",
            memory_size=1,
            dimensions=1024,
            notes="open_clip models",
            pretrained="openai",
        )),
        "open_clip/RN50-quickgelu/yfcc15m": vars(OpenClipModelProperties(
            name="open_clip/RN50-quickgelu/yfcc15m",
            memory_size=1,
            dimensions=1024,
            notes="open_clip models",
            pretrained="yfcc15m",
        )),
        "open_clip/RN50-quickgelu/cc12m": vars(OpenClipModelProperties(
            name="open_clip/RN50-quickgelu/cc12m",
            memory_size=1,
            dimensions=1024,
            notes="open_clip models",
            pretrained="cc12m",
        )),
        "open_clip/RN101/openai": vars(OpenClipModelProperties(
            name="open_clip/RN101/openai",
            memory_size=1,
            dimensions=512,
            notes="open_clip models",
            pretrained="openai",
        )),
        "open_clip/RN101/yfcc15m": vars(OpenClipModelProperties(
            name="open_clip/RN101/yfcc15m",
            memory_size=1,
            dimensions=512,
            notes="open_clip models",
            pretrained="yfcc15m",
        )),
        "open_clip/RN101-quickgelu/openai": vars(OpenClipModelProperties(
            name="open_clip/RN101-quickgelu/openai",
            memory_size=1,
            dimensions=512,
            notes="open_clip models",
            pretrained="openai",
        )),
        "open_clip/RN101-quickgelu/yfcc15m": vars(OpenClipModelProperties(
            name="open_clip/RN101-quickgelu/yfcc15m",
            memory_size=1,
            dimensions=512,
            notes="open_clip models",
            pretrained="yfcc15m",
        )),
        "open_clip/RN50x4/openai": vars(OpenClipModelProperties(
            name="open_clip/RN50x4/openai",
            memory_size=1,
            dimensions=640,
            notes="open_clip models",
            pretrained="openai",
        )),
        "open_clip/RN50x16/openai": vars(OpenClipModelProperties(
            name="open_clip/RN50x16/openai",
            memory_size=1,
            dimensions=768,
            notes="open_clip models",
            pretrained="openai",
        )),
        "open_clip/RN50x64/openai": vars(OpenClipModelProperties(
            name="open_clip/RN50x64/openai",
            memory_size=1,
            dimensions=1024,
            notes="open_clip models",
            pretrained="openai",
        )),
        "open_clip/ViT-B-32/openai": vars(OpenClipModelProperties(
            name="open_clip/ViT-B-32/openai",
            memory_size=1,
            dimensions=512,
            notes="open_clip models",
            pretrained="openai",
        )),
        "open_clip/ViT-B-32/laion400m_e31": vars(OpenClipModelProperties(
            name="open_clip/ViT-B-32/laion400m_e31",
            memory_size=1,
            dimensions=512,
            notes="open_clip models",
            pretrained="laion400m_e31",
        )),
        "open_clip/ViT-B-32/laion400m_e32": vars(OpenClipModelProperties(
            name="open_clip/ViT-B-32/laion400m_e32",
            memory_size=1,
            dimensions=512,
            notes="open_clip models",
            pretrained="laion400m_e32",
        )),
        "open_clip/ViT-B-32/laion2b_e16": vars(OpenClipModelProperties(
            name="open_clip/ViT-B-32/laion2b_e16",
            memory_size=1,
            dimensions=512,
            notes="open_clip models",
            pretrained="laion2b_e16",
        )),
        "open_clip/ViT-B-32/laion2b_s34b_b79k": vars(OpenClipModelProperties(
            name="open_clip/ViT-B-32/laion2b_s34b_b79k",
            memory_size=1,
            dimensions=512,
            notes="open_clip models",
            pretrained="laion2b_s34b_b79k",
        )),
        "open_clip/ViT-B-32-quickgelu/openai": vars(OpenClipModelProperties(
            name="open_clip/ViT-B-32-quickgelu/openai",
            memory_size=1,
            dimensions=512,
            notes="open_clip models",
            pretrained="openai",
        )),
        "open_clip/ViT-B-32-quickgelu/laion400m_e31": vars(OpenClipModelProperties(
            name="open_clip/ViT-B-32-quickgelu/laion400m_e31",
            memory_size=1,
            dimensions=512,
            notes="open_clip models",
            pretrained="laion400m_e31",
        )),
        "open_clip/ViT-B-32-quickgelu/laion400m_e32": vars(OpenClipModelProperties(
            name="open_clip/ViT-B-32-quickgelu/laion400m_e32",
            memory_size=1,
            dimensions=512,
            notes="open_clip models",
            pretrained="laion400m_e32",
        )),
        "open_clip/ViT-B-16/openai": vars(OpenClipModelProperties(
            name="open_clip/ViT-B-16/openai",
            memory_size=1,
            dimensions=512,
            notes="open_clip models",
            pretrained="openai",
        )),
        "open_clip/ViT-B-16/laion400m_e31": vars(OpenClipModelProperties(
            name="open_clip/ViT-B-16/laion400m_e31",
            memory_size=1,
            dimensions=512,
            notes="open_clip models",
            pretrained="laion400m_e31",
        )),
        "open_clip/ViT-B-16/laion400m_e32": vars(OpenClipModelProperties(
            name="open_clip/ViT-B-16/laion400m_e32",
            memory_size=1,
            dimensions=512,
            notes="open_clip models",
            pretrained="laion400m_e32",
        )),
        "open_clip/ViT-B-16/laion2b_s34b_b88k": vars(OpenClipModelProperties(
            name="open_clip/ViT-B-16/laion2b_s34b_b88k",
            memory_size=1,
            dimensions=512,
            notes="open_clip models",
            pretrained="laion2b_s34b_b88k",
        )),
        "open_clip/ViT-B-16-plus-240/laion400m_e31": vars(OpenClipModelProperties(
            name="open_clip/ViT-B-16-plus-240/laion400m_e31",
            memory_size=1,
            dimensions=640,
            notes="open_clip models",
            pretrained="laion400m_e31",
        )),
        "open_clip/ViT-B-16-plus-240/laion400m_e32": vars(OpenClipModelProperties(
            name="open_clip/ViT-B-16-plus-240/laion400m_e32",
            memory_size=1,
            dimensions=640,
            notes="open_clip models",
            pretrained="laion400m_e32",
        )),
        "open_clip/ViT-L-14/openai": vars(OpenClipModelProperties(
            name="open_clip/ViT-L-14/openai",
            memory_size=1.5,
            dimensions=768,
            notes="open_clip models",
            pretrained="openai",
        )),
        "open_clip/ViT-L-14/laion400m_e31": vars(OpenClipModelProperties(
            name="open_clip/ViT-L-14/laion400m_e31",
            memory_size=1.5,
            dimensions=768,
            notes="open_clip models",
            pretrained="laion400m_e31",
        )),
        "open_clip/ViT-L-14/laion400m_e32": vars(OpenClipModelProperties(
            name="open_clip/ViT-L-14/laion400m_e32",
            memory_size=1.5,
            dimensions=768,
            notes="open_clip models",
            pretrained="laion400m_e32",
        )),
        "open_clip/ViT-L-14/laion2b_s32b_b82k": vars(OpenClipModelProperties(
            name="open_clip/ViT-L-14/laion2b_s32b_b82k",
            memory_size=1.5,
            dimensions=768,
            notes="open_clip models",
            pretrained="laion2b_s32b_b82k",
        )),
        "open_clip/ViT-L-14-336/openai": vars(OpenClipModelProperties(
            name="open_clip/ViT-L-14-336/openai",
            memory_size=1.5,
            dimensions=768,
            notes="open_clip models",
            pretrained="openai",
        )),
        "open_clip/ViT-H-14/laion2b_s32b_b79k": vars(OpenClipModelProperties(
            name="open_clip/ViT-H-14/laion2b_s32b_b79k",
            memory_size=5,
            dimensions=1024,
            notes="open_clip models",
            pretrained="laion2b_s32b_b79k",
        )),
        "open_clip/ViT-g-14/laion2b_s12b_b42k": vars(OpenClipModelProperties(
            name="open_clip/ViT-g-14/laion2b_s12b_b42k",
            memory_size=5,
            dimensions=1024,
            notes="open_clip models",
            pretrained="laion2b_s12b_b42k",
        )),
        "open_clip/ViT-g-14/laion2b_s34b_b88k": vars(OpenClipModelProperties(
            name="open_clip/ViT-g-14/laion2b_s34b_b88k",
            memory_size=5,
            dimensions=1024,
            notes="open_clip models",
            pretrained="laion2b_s34b_b88k",
        )),
        "open_clip/ViT-bigG-14/laion2b_s39b_b160k": vars(OpenClipModelProperties(
            name="open_clip/ViT-bigG-14/laion2b_s39b_b160k",
            memory_size=6,
            dimensions=1280,
            notes="open_clip models",
            pretrained="laion2b_s39b_b160k",
        )),
        "open_clip/roberta-ViT-B-32/laion2b_s12b_b32k": vars(OpenClipModelProperties(
            name="open_clip/roberta-ViT-B-32/laion2b_s12b_b32k",
            memory_size=1,
            dimensions=512,
            notes="open_clip models",
            pretrained="laion2b_s12b_b32k",
        )),
        "open_clip/xlm-roberta-base-ViT-B-32/laion5b_s13b_b90k": vars(OpenClipModelProperties(
            name="open_clip/xlm-roberta-base-ViT-B-32/laion5b_s13b_b90k",
            memory_size=1,
            dimensions=512,
            notes="open_clip models",
            pretrained="laion5b_s13b_b90k",
        )),
        "open_clip/xlm-roberta-large-ViT-H-14/frozen_laion5b_s13b_b90k": vars(OpenClipModelProperties(
            name="open_clip/xlm-roberta-large-ViT-H-14/frozen_laion5b_s13b_b90k",
            memory_size=5,
            dimensions=1024,
            notes="open_clip models",
            pretrained="frozen_laion5b_s13b_b90k",
        )),
        "open_clip/convnext_base/laion400m_s13b_b51k": vars(OpenClipModelProperties(
            name="open_clip/convnext_base/laion400m_s13b_b51k",
            memory_size=1,
            dimensions=512,
            notes="open_clip models",
            pretrained="laion400m_s13b_b51k",
        )),
        "open_clip/convnext_base_w/laion2b_s13b_b82k": vars(OpenClipModelProperties(
            name="open_clip/convnext_base_w/laion2b_s13b_b82k",
            memory_size=1,
            dimensions=640,
            notes="open_clip models",
            pretrained="laion2b_s13b_b82k",
        )),
        "open_clip/convnext_base_w/laion2b_s13b_b82k_augreg": vars(OpenClipModelProperties(
            name="open_clip/convnext_base_w/laion2b_s13b_b82k_augreg",
            memory_size=1,
            dimensions=640,
            notes="open_clip models",
            pretrained="laion2b_s13b_b82k_augreg",
        )),
        "open_clip/convnext_base_w/laion_aesthetic_s13b_b82k": vars(OpenClipModelProperties(
            name="open_clip/convnext_base_w/laion_aesthetic_s13b_b82k",
            memory_size=1,
            dimensions=640,
            notes="open_clip models",
            pretrained="laion_aesthetic_s13b_b82k",
        )),
        "open_clip/convnext_base_w_320/laion_aesthetic_s13b_b82k": vars(OpenClipModelProperties(
            name="open_clip/convnext_base_w_320/laion_aesthetic_s13b_b82k",
            memory_size=1,
            dimensions=640,
            notes="open_clip models",
            pretrained="laion_aesthetic_s13b_b82k",
        )),
        "open_clip/convnext_base_w_320/laion_aesthetic_s13b_b82k_augreg": vars(OpenClipModelProperties(
            name="open_clip/convnext_base_w_320/laion_aesthetic_s13b_b82k_augreg",
            memory_size=1,
            dimensions=640,
            notes="open_clip models",
            pretrained="laion_aesthetic_s13b_b82k_augreg",
        )),
        "open_clip/convnext_large_d/laion2b_s26b_b102k_augreg": vars(OpenClipModelProperties(
            name="open_clip/convnext_large_d/laion2b_s26b_b102k_augreg",
            memory_size=1,
            dimensions=768,
            notes="open_clip models",
            pretrained="laion2b_s26b_b102k_augreg",
        )),
        "open_clip/convnext_large_d_320/laion2b_s29b_b131k_ft": vars(OpenClipModelProperties(
            name="open_clip/convnext_large_d_320/laion2b_s29b_b131k_ft",
            memory_size=1,
            dimensions=768,
            notes="open_clip models",
            pretrained="laion2b_s29b_b131k_ft",
        )),
        "open_clip/convnext_large_d_320/laion2b_s29b_b131k_ft_soup": vars(OpenClipModelProperties(
            name="open_clip/convnext_large_d_320/laion2b_s29b_b131k_ft_soup",
            memory_size=1,
            dimensions=768,
            notes="open_clip models",
            pretrained="laion2b_s29b_b131k_ft_soup",
        )),
        # Comment out as they are not currently available in open_clip release 2.18.1
        # It is discussed here https: // github.com / mlfoundations / open_clip / issues / 477
        # "open_clip/convnext_xxlarge/laion2b_s34b_b82k_augreg": vars(OpenClipProperties(
        #     name="open_clip/convnext_xxlarge/laion2b_s34b_b82k_augreg",
        #     dimensions=1024,
        #     notes="open_clip models",
        #     pretrained="laion2b_s34b_b82k_augreg",
        # )),
        # "open_clip/convnext_xxlarge/laion2b_s34b_b82k_augreg_rewind": vars(OpenClipProperties(
        #     name="open_clip/convnext_xxlarge/laion2b_s34b_b82k_augreg_rewind",
        #     dimensions=1024,
        #     notes="open_clip models",
        #     pretrained="laion2b_s34b_b82k_augreg_rewind",
        # )),
        # "open_clip/convnext_xxlarge/laion2b_s34b_b82k_augreg_soup": vars(OpenClipProperties(
        #     name="open_clip/convnext_xxlarge/laion2b_s34b_b82k_augreg_soup",
        #     dimensions=1024,
        #     notes="open_clip models",
        #     pretrained="laion2b_s34b_b82k_augreg_soup",
        # )),
        "open_clip/coca_ViT-B-32/laion2b_s13b_b90k": vars(OpenClipModelProperties(
            name="open_clip/coca_ViT-B-32/laion2b_s13b_b90k",
            memory_size=1,
            dimensions=512,
            notes="open_clip models",
            pretrained="laion2b_s13b_b90k",
        )),
        "open_clip/coca_ViT-B-32/mscoco_finetuned_laion2b_s13b_b90k": vars(OpenClipModelProperties(
            name="open_clip/coca_ViT-B-32/mscoco_finetuned_laion2b_s13b_b90k",
            memory_size=1,
            dimensions=512,
            notes="open_clip models",
            pretrained="mscoco_finetuned_laion2b_s13b_b90k",
        )),
        "open_clip/coca_ViT-L-14/laion2b_s13b_b90k": vars(OpenClipModelProperties(
            name="open_clip/coca_ViT-L-14/laion2b_s13b_b90k",
            memory_size=1.5,
            dimensions=768,
            notes="open_clip models",
            pretrained="laion2b_s13b_b90k",
        )),
        "open_clip/coca_ViT-L-14/mscoco_finetuned_laion2b_s13b_b90k": vars(OpenClipModelProperties(
            name="open_clip/coca_ViT-L-14/mscoco_finetuned_laion2b_s13b_b90k",
            memory_size=1.5,
            dimensions=768,
            notes="open_clip models",
            pretrained="mscoco_finetuned_laion2b_s13b_b90k",
        )),
    }
    return OPEN_CLIP_MODEL_PROPERTIES
